{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (1.5.16)\n",
      "Requirement already satisfied: six>=1.10 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (1.26.15)\n",
      "Requirement already satisfied: bleach in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: webencodings in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->kaggle) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->kaggle) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your kaggle api token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/meetbanthia/.kaggle/kaggle.json'\n",
      "ref                                                           title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "------------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "snap/amazon-fine-food-reviews                                 Amazon Fine Food Reviews                           242MB  2017-05-01 18:51:31         169078       2151  0.7941176        \n",
      "eswarchandt/amazon-music-reviews                              Amazon Musical Instruments Reviews                   5MB  2020-03-29 02:59:52          15402        293  1.0              \n",
      "kritanjalijain/amazon-reviews                                 Amazon reviews                                       1GB  2021-05-15 09:45:40          10854        127  1.0              \n",
      "grikomsn/amazon-cell-phones-reviews                           Amazon Cell Phones Reviews                           9MB  2019-12-26 22:21:16          17696        220  1.0              \n",
      "sid321axn/amazon-alexa-reviews                                Amazon Alexa Reviews                               164KB  2018-07-31 17:45:14          21326        264  0.8235294        \n",
      "mohamedbakhet/amazon-books-reviews                            Amazon Books Reviews                                 1GB  2022-09-13 23:04:08           9738        129  1.0              \n",
      "bittlingmayer/amazonreviews                                   Amazon Reviews for Sentiment Analysis              493MB  2019-11-18 02:50:34          67860        901  0.6875           \n",
      "PromptCloudHQ/amazon-reviews-unlocked-mobile-phones           Amazon Reviews: Unlocked Mobile Phones              33MB  2017-01-11 10:22:30          14298        176  1.0              \n",
      "datafiniti/consumer-reviews-of-amazon-products                Consumer Reviews of Amazon Products                 16MB  2019-05-20 00:38:59          43470        452  0.7647059        \n",
      "bharadwaj6/kindle-reviews                                     Amazon reviews: Kindle Store Category              525MB  2018-05-22 04:50:16           7025        164  1.0              \n",
      "abdallahwagih/amazon-reviews                                  Amazon reviews                                      44MB  2023-10-16 18:25:58           1315         56  1.0              \n",
      "yasserh/amazon-product-reviews-dataset                        Amazon Product Reviews Dataset                     708KB  2022-01-23 17:21:16           5067         64  1.0              \n",
      "surajjha101/myntra-reviews-on-women-dresses-comprehensive     Amazon reviews on Women dresses (23K Datapoints)     3MB  2022-06-14 10:29:31           2183        114  1.0              \n",
      "saurav9786/amazon-product-reviews                             Amazon Product Reviews                             109MB  2020-01-14 13:25:53          14298        123  0.64705884       \n",
      "arhamrumi/amazon-product-reviews                              Amazon Product Reviews                             115MB  2021-07-11 17:40:05           3809         36  1.0              \n",
      "anshtanwar/top-200-trending-books-with-reviews                Top 100 Bestselling Book Reviews on Amazon         422KB  2023-11-09 06:31:02           8240         79  1.0              \n",
      "cynthiarempel/amazon-us-customer-reviews-dataset              Amazon US Customer Reviews Dataset                  21GB  2021-06-16 20:07:46           7049         57  1.0              \n",
      "tarkkaanko/amazon                                             amazon reviews for sentiment analysis              582KB  2022-07-21 10:43:26           5164         62  0.85294116       \n",
      "shitalkat/amazonearphonesreviews                              Amazon Earphones Reviews                           772KB  2019-07-19 04:53:11           3090         29  1.0              \n",
      "meetnagadia/amazon-kindle-book-review-for-sentiment-analysis  Amazon Kindle Book Review for Sentiment Analysis     6MB  2021-09-03 11:52:29           3034         50  1.0              \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list -s \"Amazon review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/meetbanthia/.kaggle/kaggle.json'\n",
      "Downloading amazonreviews.zip to /Users/meetbanthia/Desktop/SentAnalysis\n",
      "100%|███████████████████████████████████████▉| 493M/493M [00:42<00:00, 12.8MB/s]\n",
      "100%|████████████████████████████████████████| 493M/493M [00:42<00:00, 12.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Download dataset\n",
    "!kaggle datasets download -d 'bittlingmayer/amazonreviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting zip file\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "file = './amazonreviews.zip'\n",
    "with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('dataset')\n",
    "\n",
    "import os\n",
    "os.system(\"rm amazonreviews.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.ft.txt.bz2', 'train.ft.txt.bz2']\n"
     ]
    }
   ],
   "source": [
    "##Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"./dataset\" directory.\n",
    "import os\n",
    "print(os.listdir(\"./dataset/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the text\n",
    "\n",
    "The text is held in a compressed format. Luckily, we can still read it line by line. The first word gives the label, so we have to convert that into a number and then take the rest to be the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "\n",
    "train_labels, train_texts = get_labels_and_texts('./dataset/train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('./dataset/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert to lowercase\n",
    "2. substitute non alphanumeric characters with whitespace\n",
    "3. Remove noascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    \"\"\"\n",
    "    texts parameter is list of sentences which needs to be pre-processes\n",
    "    returns list containing each item of texts preprocessed\n",
    "    \"\"\"\n",
    "    preprocessed_texts = []\n",
    "\n",
    "    #issue1 - pre-processing\n",
    "\n",
    "    return preprocessed_texts\n",
    "        \n",
    "train_texts = preprocess(train_texts)\n",
    "test_texts = preprocess(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data into train and validation data - try keeping test_size = 0.2\n",
    "# train_texts, val_texts, train_labels, val_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Tokenizer to convert text into model usable form\n",
    "\n",
    "#code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_texts ,val_texts ,test_texts now contains model usable form of texts\n",
    "\n",
    "#Do padding\n",
    "#find maxlen\n",
    "#Use padding to convert all vectors to maxlen and save them in train_texts ,val_texts ,test_texts itself\n",
    "\n",
    "#code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    #code here\n",
    "\n",
    "    #return model\n",
    "    \n",
    "lstm_model = build_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "lstm_model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lstm_model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_comment = \"Type here..\"\n",
    "\n",
    "\n",
    "lst = []\n",
    "lst.append(your_comment)\n",
    "preds = lstm_model.predict(lst)\n",
    "\n",
    "if preds[0]>0.5:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell only if dataset is no longer needed\n",
    "\n",
    "#Deleting dataset as it exceeds 100 MB, github max limit\n",
    "import os \n",
    "os.system(\"rm -r dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
